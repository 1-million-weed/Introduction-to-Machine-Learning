---
tags:
  - Marinus
  - _FirstPass
  - exam
  - Notes
  - Highlightr
Created: 2025-01-31
---
# Questions to Think About

## 1. What is a feature?
A feature is a <mark style="background: #D2B3FFA6;">measurable property, characteristic, or attribute</mark> of a phenomenon or physical object. In machine learning, features are the **input variables** used to train a model for classification or regression tasks. The quality of features directly affects model performance. 

Examples of features:
- **Images:** Pixel values, edges, colors, histograms.
- **Text:** Frequency of words, language, grammar errors.
- **Sound:** Frequency response, signal-to-noise ratio.

For best results, features should be <mark style="background: #FFB8EBA6;">de-correlated and independent.</mark> Poorly chosen features can prevent a model from learning correctly.

## 2. Describe linear separability in your own words.
Linear separability refers to a <mark style="background: #FF5582A6;">classification problem where data points belonging to different classes can be separated by a straight line (in 2D) or a hyperplane (in higher dimensions).</mark> 

Formally, a dataset is <mark style="background: #FFB86CA6;">linearly separable</mark> if there exists a weight vector $w$ and a threshold $k$ such that:

- $w \cdot x_i > k$ for all points belonging to one class.
- $w \cdot x_i < k$ for all points belonging to the other class.

If no such hyperplane exists, the data is <mark style="background: #BBFABBA6;">not linearly separable</mark>, meaning more complex decision boundaries or feature transformations are needed.

## 3. What is the need for normalization and pre-processing of features?
Normalization and pre-processing are crucial for ensuring that features have similar scales and distributions. 

**Reasons for normalization:**
- Different features may have vastly <mark style="background: #ABF7F7A6;">different numerical ranges</mark> (e.g., height in cm vs. income in dollars), which can cause models to assign unintended importance to certain features.
- Many optimization algorithms (such as gradient descent) perform <mark style="background: #ADCCFFA6;">better and converge faster</mark> when features are on a similar scale.
- Helps <mark style="background: #D2B3FFA6;">reduce bias</mark> in learning by ensuring all features contribute proportionally.

**Common [[Normalisation]] techniques:**
- **Min-Max Scaling:** Rescales data to a $[0,1]$ range.
- **Standardization (Z-score):** Rescales data to have a mean of 0 and a standard deviation of 1.
- **Whitening (PCA-based):** De-correlates features and sets covariance to the identity matrix.

## 4. Can regression labels/targets be normalized?
<mark style="background: #BBFABBA6;">Yes</mark>, regression labels (targets) can and often **should** be normalized. 

**Benefits of normalizing regression labels:**
- Makes it easier for the model to <mark style="background: #D2B3FFA6;">learn relationships</mark> between inputs and outputs.
- Helps models with certain activation functions (e.g., sigmoid, tanh) stay within predictable output ranges.
- Can **prevent extreme values** from dominating the learning process.
- Standardizing labels ensures the model outputs **values within a reasonable range**, preventing prediction drift.

## 5. What factors can hurt the performance of a Machine Learning model?
Several factors can negatively impact machine learning model performance:

1. **Poor Data Quality:** Incorrect labels, missing values, or noisy features can mislead the model. [[Quality of Data]]
2. **Overfitting:** When a model learns **too much from the training data**, it fails to generalize to unseen data.
3. **Underfitting:** The model is too simple and fails to capture patterns in the data.
4. **Incorrect Feature Selection:** Redundant or irrelevant features can introduce noise.
5. **Improper [[Normalisation]]:** Features with vastly different scales can cause bias in learning.
6. [[Model Misspecification]]: Choosing an incorrect model for the given dataset. 
7. **Lack of [[Generalization]]:** The model works well on training data but fails on new data.

## 6. Your model does not learn, what is the most likely cause?
If a machine learning model fails to learn, the most common causes include:

1. **Bad Data Quality:** The dataset may contain errors, missing values, or irrelevant features. [[Quality of Data]]
2. **Incorrect Hyperparameters:** Learning rate too high (causing instability) or too low (causing slow convergence).
3. **Poor Feature Selection:** Features may be uninformative or correlated. 
4. **Lack of Sufficient Data:** A model needs enough data to generalize well.
5. **Incorrect Model Choice:** The chosen model may be too simple (underfitting) or too complex without enough data. 
6. **Optimization Issues:** Poor choice of loss function or optimizer may prevent effective learning.

---

These answers are based on the content of the provided lecture notes on machine learning.
