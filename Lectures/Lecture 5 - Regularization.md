---
tags:
  - _FirstPass
  - Marinus
  - Lectures
  - _SecondPass
DateReviewed1: 2024-12-12
---

# Regularization

- apply techniques to generalise model
- how do we add additional information to the model 
	- llm, you give context and the model uses that

> Regularisation
> contrain model to limit its learning capacity or complexity so it is under teh control of the ML user


> Regularizer

> Regularizing Effect

- General Patterns
	- Weight constraints
	- Output Constraints
	- noise
- when regularizing a model the complexity and capacity P changes
- Strength parameter
	- controls complexity - how strong or weak the regularization effect is 
	- new model complexity/capacity is called the effective P

- large R
	- very constrained and effective P decreases 

Lp regularisation
- constrains the model considerably

L1
- Lasso

I am sorry, i am exhausted today. I was down hard on the floor.

L2 is called ridge regularisation on a linear model

slide 37 implicit or indirect regularization

basically do more and  its better

